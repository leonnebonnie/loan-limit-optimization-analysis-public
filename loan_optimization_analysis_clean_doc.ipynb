{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37db959",
   "metadata": {},
   "source": [
    "# Loan Limit Optimization â€” Documented Notebook\n",
    "This notebook reproduces the cleaned analysis pipeline but adds inline explanations so a technical reviewer can follow the steps.\n",
    "Cells are grouped: setup, constants, data loading, feature engineering, models, optimization, simulation, and outputs.\n",
    "Keep the input file `loan_limit_increases.xlsx` in the same folder before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e7d87",
   "metadata": {},
   "source": [
    "## Setup: imports and reproducibility\n",
    "We import common data and ML libraries and fix random seeds so runs are repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4ede65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library + numeric/data libs\n",
    "import os\n",
    "import random\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Scikit-learn models and utilities\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Reproducibility: set an explicit seed for random and numpy to make results deterministic where possible\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Note: some algorithms (e.g., multithreaded trees) may still introduce nondeterminism across platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34866d2f",
   "metadata": {},
   "source": [
    "## Constants and filenames\n",
    "Keep these central so the notebook is easy to adjust for different simulation sizes or output names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc440022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business and model constants (kept from the original script)\n",
    "PROFIT_PER_INCREASE = 40  # profit recognized per successful increase (assumption)\n",
    "MAX_INCREASES_PER_YEAR = 6  # operational or policy cap per customer\n",
    "DISCOUNT_RATE = 0.19  # annual discount rate used for NPV calculations\n",
    "ELIGIBILITY_THRESHOLD_DAYS = 60  # days since last loan required to be eligible\n",
    "\n",
    "# Filenames used for input/output in the project\n",
    "INPUT_FILE = 'loan_limit_increases.xlsx'  # expected Excel in working dir\n",
    "OUT_RESULTS = 'loan_optimization_results.csv'\n",
    "OUT_RECOMM = 'recommended_increases.csv'\n",
    "OUT_SIM = 'simulation_results.csv'\n",
    "\n",
    "# Small note: you can change these names if you want multiple experiment outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69550372",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Load the Excel input and perform a small header normalization step to handle files where the first row contains human-readable headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d63ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the dataset from Excel and coerce numeric columns.\n",
    "    The function also detects when the first row contains header labels exported from other tools and corrects that.\n",
    "    Returns: DataFrame with numeric conversions applied where possible.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
    "    # Read the sheet into a DataFrame; keep all rows, we'll detect header issues below\n",
    "    df = pd.read_excel(path, skiprows=0)\n",
    "    # Robustness: sometimes Excel exports put the true header on the first row. Detect and fix it\n",
    "    if 'Customer ID' not in df.columns:\n",
    "        first_row = df.iloc[0].astype(str).str.lower()\n",
    "        if first_row.str.contains('customer id').any():\n",
    "            # promote the first row to be the header and drop it from data rows\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df.iloc[1:].reset_index(drop=True)\n",
    "    # Coerce numeric columns where appropriate to avoid dtype surprises later\n",
    "    for col in df.columns:\n",
    "        if col != 'Customer ID':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffcc120",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "We add simple derived features used by the uptake and default models. These are intentionally straightforward so the reasoning is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08969f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Work on a copy so we don't modify the caller's DataFrame in place\n",
    "    df = df.copy()\n",
    "    # Eligibility based on policy (days since last loan)\n",
    "    df['Eligible'] = (df['Days Since Last Loan'] >= ELIGIBILITY_THRESHOLD_DAYS).astype(int)\n",
    "    # Historical indicator: did the customer receive an increase in the prior year?\n",
    "    df['Received_Increase'] = (df['No. of Increases in 2023'] > 0).astype(int)\n",
    "    # Simple risk bucketing from on-time payment percent\n",
    "    def assign_risk(payment_rate):\n",
    "        if payment_rate >= 95: return 'Prime'\n",
    "        if payment_rate >= 85: return 'Near-Prime'\n",
    "        return 'Sub-Prime'\n",
    "    df['Risk_Category'] = df['On-time Payments (%)'].apply(assign_risk)\n",
    "    # Loan size buckets can help tree-based models capture scale effects\n",
    "    df['Loan_Size_Category'] = pd.cut(df['Initial Loan ($)'], bins=[0,1500,3000,5000], labels=['Small','Medium','Large'])\n",
    "    # A proxy credit score synthesized from available columns (for demo / modeling purposes)\n",
    "    df['Credit_Score_Proxy'] = (\n",
    "        df['On-time Payments (%)'] * 0.6 +\n",
    "        (df['Days Since Last Loan'] / df['Days Since Last Loan'].max() * 100) * 0.2 +\n",
    "        ((df['Initial Loan ($)'] / df['Initial Loan ($)'].max()) * 100) * 0.2\n",
    "    )\n",
    "    # Interaction features that might improve model signals\n",
    "    df['Payment_Days_Interaction'] = df['On-time Payments (%)'] * df['Days Since Last Loan'] / 100\n",
    "    df['Loan_Payment_Ratio'] = df['Initial Loan ($)'] / (df['On-time Payments (%)'] + 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e950f",
   "metadata": {},
   "source": [
    "## Uptake modeling (who accepts an increase)\n",
    "We train a few candidate models and pick the best by AUC. The chosen model is used to predict an `Uptake_Probability` for every customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b200df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uptake_models(df: pd.DataFrame):\n",
    "    # Feature list used by the uptake models (kept concise)\n",
    "    features = ['Initial Loan ($)','Days Since Last Loan','On-time Payments (%)','Credit_Score_Proxy','Payment_Days_Interaction','Loan_Payment_Ratio']\n",
    "    X = df[features]\n",
    "    y = df['Received_Increase']  # historical indicator used as the training target\n",
    "    # Split for model selection; stratify keeps class balance in train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
    "    # Standardize only where models expect scaled inputs (logistic regression)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_sc = scaler.fit_transform(X_train)\n",
    "    X_test_sc = scaler.transform(X_test)\n",
    "    # Fit three models (simple baseline + trees)\n",
    "    lr = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)\n",
    "    lr.fit(X_train_sc, y_train)\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, max_depth=10)\n",
    "    rf.fit(X_train, y_train)  # trees don't need scaling\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_SEED, max_depth=5)\n",
    "    gb.fit(X_train, y_train)\n",
    "    models = {'Logistic Regression': (lr, X_test_sc), 'Random Forest': (rf, X_test), 'Gradient Boosting': (gb, X_test)}\n",
    "    # Evaluate AUC on the test split to pick the best model\n",
    "    best_name, best_auc, best_model = None, -1.0, None\n",
    "    for name, (model, Xt) in models.items():\n",
    "        try:\n",
    "            proba = model.predict_proba(Xt)[:,1]\n",
    "            auc = roc_auc_score(y_test, proba)\n",
    "        except Exception:\n",
    "            auc = 0.0\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_name = name\n",
    "            best_model = model\n",
    "    # Use the best model to predict uptake probability for all rows\n",
    "    if best_name == 'Logistic Regression':\n",
    "        df['Uptake_Probability'] = lr.predict_proba(scaler.transform(df[features]))[:,1]\n",
    "    else:\n",
    "        df['Uptake_Probability'] = best_model.predict_proba(df[features])[:,1]\n",
    "    return best_name, best_auc, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47215c9",
   "metadata": {},
   "source": [
    "## Default risk modeling\n",
    "We create a simple heuristic default score and map it to a probability using a logistic function. This is intentionally interpretable rather than a black-box model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_default_model(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Compose a risk score from observable signals (higher = worse)\n",
    "    df['Default_Risk_Score'] = (\n",
    "        (100 - df['On-time Payments (%)']) * 0.5 +\n",
    "        (100 - df['Credit_Score_Proxy']) * 0.3 +\n",
    "        (df['Initial Loan ($)'] / df['Initial Loan ($)'].max() * 100) * 0.2\n",
    "    )\n",
    "    # Map score to probability (sigmoid); this yields values in (0,1)\n",
    "    df['Default_Probability'] = 1 / (1 + np.exp(-0.1 * (df['Default_Risk_Score'] - 50)))\n",
    "    # Heuristic: previous increases slightly raise default risk\n",
    "    df['Adjusted_Default_Probability'] = df['Default_Probability'] * (1 + 0.05 * df['No. of Increases in 2023'])\n",
    "    df['Adjusted_Default_Probability'] = df['Adjusted_Default_Probability'].clip(0, 0.95)  # cap at 95%\n",
    "    return df\n",
    "\n",
    "def calculate_expected_value(row):\n",
    "    # Expected value (per potential increase) = profit * P(uptake) * (1 - P(default)) - expected loss from defaults\n",
    "    uptake_prob = row['Uptake_Probability']\n",
    "    default_prob = row['Adjusted_Default_Probability']\n",
    "    expected_profit = PROFIT_PER_INCREASE * uptake_prob * (1 - default_prob)\n",
    "    expected_loss = row['Initial Loan ($)'] * 0.5 * uptake_prob * default_prob\n",
    "    return expected_profit - expected_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210773b",
   "metadata": {},
   "source": [
    "## Optimization heuristic\n",
    "A greedy approach: rank eligible customers by a risk-adjusted expected value and recommend a small number of increases per customer while respecting high-risk limits and an optional capital constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64465cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_loan_increases(df_input, max_high_risk_pct=0.25, capital_constraint=None):\n",
    "    # Work only on eligible customers\n",
    "    eligible = df_input[df_input['Eligible'] == 1].copy()\n",
    "    # Sort by a risk-adjusted score so top rows are prioritized\n",
    "    eligible = eligible.sort_values('Risk_Adjusted_Score', ascending=False)\n",
    "    eligible['Recommended_Increases'] = 0\n",
    "    eligible['Total_Expected_Value'] = 0.0\n",
    "    total_value = total_exposure = 0.0\n",
    "    high_risk_count = total_approvals = 0\n",
    "    # Iterate and greedily allocate increases when expected value is positive\n",
    "    for idx, row in eligible.iterrows():\n",
    "        is_high_risk = row['Risk_Category'] == 'Sub-Prime'\n",
    "        # enforce a maximum percentage of approvals that can be high-risk\n",
    "        if is_high_risk and high_risk_count >= len(eligible) * max_high_risk_pct:\n",
    "            continue\n",
    "        if row['Expected_Value'] <= 0:  # skip negative expectation rows\n",
    "            continue\n",
    "        # choose a reasonable number of increases based on predicted uptake\n",
    "        optimal_increases = min(MAX_INCREASES_PER_YEAR, int(row['Uptake_Probability'] * MAX_INCREASES_PER_YEAR) + 1)\n",
    "        if capital_constraint:\n",
    "            projected_exposure = row['Initial Loan ($)'] * optimal_increases * 0.5\n",
    "            if total_exposure + projected_exposure > capital_constraint:\n",
    "                continue\n",
    "        eligible.at[idx, 'Recommended_Increases'] = optimal_increases\n",
    "        eligible.at[idx, 'Total_Expected_Value'] = float(row['Expected_Value']) * optimal_increases\n",
    "        total_value += eligible.at[idx, 'Total_Expected_Value']\n",
    "        total_exposure += row['Initial Loan ($)'] * optimal_increases * 0.5\n",
    "        if is_high_risk:\n",
    "            high_risk_count += 1\n",
    "        total_approvals += 1\n",
    "    return {'eligible_df': eligible, 'total_expected_value': total_value, 'total_approvals': total_approvals, 'total_exposure': total_exposure, 'high_risk_count': high_risk_count, 'high_risk_pct': high_risk_count / total_approvals if total_approvals > 0 else 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e82a3b",
   "metadata": {},
   "source": [
    "## Monte Carlo simulation (lifecycle per customer)\n",
    "Simulate transitions between risk states and whether increases are accepted and whether defaults occur. This estimates the distribution of net value per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_loan_lifecycle(customer_row, n_simulations=100, time_periods=4):\n",
    "    # We'll collect a small results table per simulation run for each customer\n",
    "    results = []\n",
    "    # Transition probabilities between risk buckets (rows sum to 1)\n",
    "    transition_matrix = np.array([[0.85,0.12,0.03],[0.15,0.7,0.15],[0.05,0.25,0.7]])\n",
    "    risk_states = ['Prime','Near-Prime','Sub-Prime']\n",
    "    for sim in range(n_simulations):\n",
    "        total_profit = total_losses = 0.0\n",
    "        defaults = increases = 0\n",
    "        # start from the customer's observed risk category\n",
    "        current_risk_state = customer_row['Risk_Category']\n",
    "        for quarter in range(time_periods):\n",
    "            if quarter > 0:\n",
    "                # sample next state using the transition matrix\n",
    "                state_idx = {'Prime':0,'Near-Prime':1,'Sub-Prime':2}[current_risk_state]\n",
    "                probs = transition_matrix[state_idx]\n",
    "                next_state_idx = np.random.choice(3, p=probs)\n",
    "                current_risk_state = risk_states[next_state_idx]\n",
    "            # risk multiplier increases the default probability for worse buckets\n",
    "            risk_multiplier = {'Prime':0.8,'Near-Prime':1.0,'Sub-Prime':1.3}[current_risk_state]\n",
    "            adjusted_default_prob = min(customer_row['Default_Probability'] * risk_multiplier, 0.95)\n",
    "            # check whether customer accepts the product this quarter\n",
    "            accepts = np.random.random() < customer_row['Uptake_Probability']\n",
    "            if accepts:\n",
    "                increases += 1\n",
    "                if np.random.random() < adjusted_default_prob:\n",
    "                    defaults += 1\n",
    "                    # assume a 50% loss given default on outstanding amount\n",
    "                    total_losses += customer_row['Initial Loan ($)'] * 0.5\n",
    "                else:\n",
    "                    total_profit += PROFIT_PER_INCREASE\n",
    "        results.append({'simulation': sim, 'total_profit': total_profit, 'total_losses': total_losses, 'net_value': total_profit - total_losses, 'defaults': defaults, 'increases_granted': increases})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def calculate_npv(cash_flows, discount_rate=DISCOUNT_RATE):\n",
    "    # Present-value calculation with quarterly discounting approximation\n",
    "    npv = 0.0\n",
    "    for t, cf in enumerate(cash_flows):\n",
    "        npv += cf / ((1 + discount_rate) ** (t / 4))\n",
    "    return npv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52db48a",
   "metadata": {},
   "source": [
    "## Run the pipeline (example)\n",
    "Adjust `SIM_CUSTOMER_COUNT`, `N_SIMULATIONS_PER_CUSTOMER`, and `SIM_QUARTERS` below before executing this cell to change computational footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example runtime parameters\n",
    "SIM_CUSTOMER_COUNT = 1000  # number of unique customers to sample for MC\n",
    "N_SIMULATIONS_PER_CUSTOMER = 100  # Monte Carlo runs per sampled customer\n",
    "SIM_QUARTERS = 4  # number of quarters to simulate\n",
    "\n",
    "# Load and prepare data\n",
    "df = load_data(INPUT_FILE)\n",
    "print('Dataset Shape:', df.shape)\n",
    "df = feature_engineering(df)\n",
    "print('Eligible Customers (>=60 days):', int(df['Eligible'].sum()))\n",
    "\n",
    "# Fit uptake models and predict probabilities\n",
    "best_name, best_auc, df = build_uptake_models(df)\n",
    "print('Best uptake model:', best_name, 'AUC=', round(best_auc,4))\n",
    "# Build default model and adjusted default probabilities\n",
    "df = build_default_model(df)\n",
    "\n",
    "# Compute expected value per potential increase and a risk-adjusted prioritization score\n",
    "df['Expected_Value'] = df.apply(calculate_expected_value, axis=1)\n",
    "df['Risk_Adjusted_Score'] = df['Expected_Value'] * (1 - df['Adjusted_Default_Probability']) * df['Uptake_Probability']\n",
    "\n",
    "# Run greedy allocation heuristic\n",
    "optimization_results = optimize_loan_increases(df, max_high_risk_pct=0.25)\n",
    "print('Approved for Increases:', optimization_results['total_approvals'])\n",
    "print('Total Expected Value: $', round(optimization_results['total_expected_value'],2))\n",
    "\n",
    "# Monte Carlo sampling: sample eligible customers (with replacement if sample > available)\n",
    "eligible_customers = df[df['Eligible'] == 1]\n",
    "available = len(eligible_customers)\n",
    "if SIM_CUSTOMER_COUNT <= available:\n",
    "    sample_customers = eligible_customers.sample(n=SIM_CUSTOMER_COUNT, random_state=RANDOM_SEED)\n",
    "else:\n",
    "    sample_customers = eligible_customers.sample(n=SIM_CUSTOMER_COUNT, replace=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Run simulations for each sampled customer and collect results\n",
    "sim_list = []\n",
    "for _, row in sample_customers.iterrows():\n",
    "    sim_df = simulate_loan_lifecycle(row, n_simulations=N_SIMULATIONS_PER_CUSTOMER, time_periods=SIM_QUARTERS)\n",
    "    sim_df['customer_id'] = row['Customer ID']\n",
    "    sim_df['risk_category'] = row['Risk_Category']\n",
    "    sim_list.append(sim_df)\n",
    "all_simulations = pd.concat(sim_list, ignore_index=True)\n",
    "\n",
    "print('Total Simulation Runs:', SIM_CUSTOMER_COUNT * N_SIMULATIONS_PER_CUSTOMER)\n",
    "print('Total Individual Decisions:', SIM_CUSTOMER_COUNT * N_SIMULATIONS_PER_CUSTOMER * SIM_QUARTERS)\n",
    "print('Simulation sample mean net value:', round(all_simulations.groupby('customer_id')['net_value'].mean().mean(),2))\n",
    "\n",
    "# Persist outputs (CSV files) so we can use the cleaned visualization script or create figures in another notebook\n",
    "df.to_csv(OUT_RESULTS, index=False)\n",
    "optimization_results['eligible_df'][optimization_results['eligible_df']['Recommended_Increases'] > 0][['Customer ID','Risk_Category','On-time Payments (%)','Initial Loan ($)','Uptake_Probability','Default_Probability','Expected_Value','Recommended_Increases','Total_Expected_Value']].to_csv(OUT_RECOMM, index=False)\n",
    "all_simulations.to_csv(OUT_SIM, index=False)\n",
    "print('Saved outputs:', OUT_RESULTS, OUT_RECOMM, OUT_SIM)\n",
    "\n",
    "# End of pipeline run cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ced69d",
   "metadata": {},
   "source": [
    "## Next steps and tips\n",
    "- If you'd like, I can: convert the visualization script into notebook cells, run the notebook end-to-end here to validate outputs, or replace the original notebook with this documented version.\n",
    "- For large simulation footprints (e.g., millions of runs) consider running on a machine with more CPU/memory or reduce `N_SIMULATIONS_PER_CUSTOMER`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
